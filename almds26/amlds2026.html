<!DOCTYPE html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-137059345-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-137059345-1');
</script>

<style type="text/css">
      body {
        font-size: 1.13em;
        font-family: "Karla", "Helvetica", sans-serif;
        padding: 4em 1em;
        width:800px;
        margin-left:auto;
        margin-right:auto;
      }
      h1, h2, h3 {
        font-family: "Libre Baskerville", serif;
      }
      h1 {
        font-size: 1.8em;
        margin-bottom: 0.3em;
      }
      h2 {
        padding-top: 0.5em;
        font-size: 1.4em;
      }
      h3 {
        font-size: 1.2em;
      }
      p {
        font-size: 1.13em;
        line-height: 1.5;
      }
      .course-title {
        color: #0A2162;
        font-weight: bold;
        font-size: 2em;
        margin-bottom: 0.2em;
      }
      .instructor {
        font-size: 1.2em;
        margin-bottom: 1em;
      }
      ul {
        line-height: 1.6;
      }
      li {
        margin-bottom: 0.5em;
      }
      a {
        color: #0A2162;
        text-decoration: none;
      }
      a:hover {
        text-decoration: underline;
      }
      .highlight {
        background-color: #f0f0f0;
        padding: 1em;
        border-left: 3px solid #0A2162;
        margin: 1em 0;
      }
      table {
        border-collapse: collapse;
        width: 100%;
        margin: 1em 0;
      }
      th, td {
        text-align: left;
        padding: 0.5em;
        border-bottom: 1px solid #ddd;
        vertical-align: top;
      }
      th {
        background-color: #f0f0f0;
        font-weight: bold;
      }
      .small-text {
        font-size: 0.9em;
      }
      .instructor-grid {
        display: flex;
        gap: 2em;
        margin: 2em 0;
        flex-wrap: wrap;
      }
      .instructor-item {
        text-align: center;
        flex: 1;
        min-width: 200px;
      }
      .instructor-item img {
        width: 150px;
        height: 150px;
        object-fit: cover;
        border-radius: 50%;
        display: block;
        margin: 0 auto 0.5em auto;
      }
      .instructor-item .role {
        font-size: 0.9em;
        color: #666;
        margin: 0.5em 0;
      }
      .instructor-item .name {
        font-weight: bold;
        margin: 0.5em 0;
      }
</style>

<title>NYU CS-GY 6763: Algorithmic Machine Learning and Data Science - Spring 2026</title>
</head>
<body>

<div class="course-title">NYU CS-GY 6763 (3943): Algorithmic Machine Learning and Data Science</div>
<div class="instructor">Spring 2026</div>

<hr />

<h2>Course Overview</h2>
<p>
This is an advanced theory course that examines contemporary computational methods that enable machine learning and data science at scale. We will cover topics including randomized algorithms, concentration inequalities, dimensionality reduction, convex optimization, spectral methods, and compressed sensing. The course emphasizes rigorous mathematical analysis and algorithmic design.
</p>

<hr />

<h2>Instructor and Course Team</h2>

<div class="instructor-grid">
  <div class="instructor-item">
    <a href="../index.html">
      <img src="ainesh_course.png" alt="Ainesh Bakshi">
    </a>
    <div class="role">Professor</div>
    <div class="name"><a href="../index.html">Ainesh Bakshi</a></div>
    <div><a href="mailto:aineshbakshi@nyu.edu">aineshbakshi@nyu.edu</a></div>
  </div>
</div>

<div class="instructor-grid">
  <div class="instructor-item">
    <a href="https://pratyushavi.com/">
      <img src="pratyush.jpg" alt="Pratyush Avi">
    </a>
    <div class="role">Course Assistant</div>
    <div class="name"><a href="https://pratyushavi.com/">Pratyush Avi</a></div>
    <div><a href="mailto:pratyushavi@nyu.edu">pratyushavi@nyu.edu</a></div>
  </div>

  <div class="instructor-item">
    <a href="https://majid-daliri.github.io/">
      <img src="majid.jpg" alt="Majid Daliri">
    </a>
    <div class="role">Course Assistant</div>
    <div class="name"><a href="https://majid-daliri.github.io/">Majid Daliri</a></div>
    <div><a href="mailto:daliri.majid@nyu.edu">daliri.majid@nyu.edu</a></div>
  </div>

  <div class="instructor-item">
    <a href="https://giancarlo-pereira.github.io/">
      <img src="giancarlo.jpg" alt="Giancarlo Pereira">
    </a>
    <div class="role">Course Assistant</div>
    <div class="name"><a href="https://giancarlo-pereira.github.io/">Giancarlo Pereira</a></div>
    <div><a href="mailto:giancarlo.pereira@nyu.edu">giancarlo.pereira@nyu.edu</a></div>
  </div>
</div>

<p>
Office: 370 Jay St., Brooklyn, NY • Office 1104<br>
Instructor Office Hours: By appointment over zoom. Monday's 10 am to 11 am (please email at least a day in advance to schedule)
</p>

<hr />

<h2>Meeting Times and Locations</h2>
<p>
<strong>Lectures:</strong> Fridays 2:00pm–4:30pm, 6 MetroTech Room 674. 
</p>

<p>
<strong>Problem-Solving Session:</strong> Tuesdays 12:30pm, Room 826, 370 Jay Street. Led by course assistant. Weekly problem-solving and Q&A.
</p>

<p>
<strong>TA Office Hours:</strong> Wednesdays 12:00pm–1:00pm and Thursdays 12:00pm–1:00pm, 8th Floor Common area, 370 Jay Street
</p>

<p>
<strong>Reading Group:</strong> Details TBA. For students working on final projects. Open to all interested students for discussion and workshopping of research papers.
</p>

<hr />

<h2>Prerequisites</h2>
<p>
This course is mathematically rigorous and intended for graduate students and advanced undergraduates. Students should have prior coursework in machine learning, a strong background in algorithms, a solid understanding of linear algebra, experience with probability theory and random variables, and comfort with writing and understanding rigorous mathematical proofs.
</p>

<hr />

<h2>Grading</h2>
<p>
Problem Sets: 40%, Midterm Exam: 25%, Final Project OR Final Exam: 25%, Participation: 10%.
</p>

<hr />

<h2>Problem Sets</h2>
<p>
There will be four problem sets throughout the semester. All assignments must be submitted via Gradescope by 11:59pm ET on the due dates listed below.
</p>

<p>
Formatting: Students have to write solutions in LaTeX or Markdown.
</p>

<p>
Grading Policy: Each problem set will be graded as follows. For each problem you solve completely, clearly indicate it is a complete solution. After submission, you will be asked to explain one randomly selected problem from among your completely solved problems on a whiteboard. You may reference your written homework during the explanation. The grade you receive on the explanation will be applied to all of your completely solved problems. For problems you do not solve completely, you may write "I don't know" to receive 25% credit on that problem. Problems that are incomplete without writing "I don't know" receive 0% credit. There is no partial credit for incomplete solutions.
</p>

<p class="small-text">
Example: If a problem set has 5 problems and you completely solve 3 of them, one of those 3 will be randomly selected for you to explain. If you receive 90% on your explanation, you receive 90% on all 3 completely solved problems. For the remaining 2 problems, you can write "I don't know" on each to receive 25% credit, or 0% if you leave them incomplete. Your final score would be: (90% + 90% + 90% + 25% + 25%) / 5 = 64%.
</p>

<table>
  <tr>
    <th>Assignment</th>
    <th>Due Date</th>
  </tr>
  <tr>
    <td>Problem Set 1</td>
    <td>Wednesday, February 4, 2026</td>
  </tr>
  <tr>
    <td>Problem Set 2</td>
    <td>Wednesday, February 25, 2026</td>
  </tr>
  <tr>
    <td>Problem Set 3</td>
    <td>Wednesday, March 25, 2026</td>
  </tr>
  <tr>
    <td>Problem Set 4</td>
    <td>Wednesday, April 22, 2026</td>
  </tr>
</table>

<div class="highlight">
Collaboration Policy: Collaboration is allowed on homework problems, but solutions and code must be written independently. Writing should not be done in parallel. You must list all collaborators separately for each problem.
<br><br>
Use of External Results: Unless otherwise stated, referencing non-standard theorems and proofs not given in class or previous problems is not allowed. All solutions must be proven from first principles. 
</div>

<hr />

<h2>Exams</h2>
<p>
Midterm Exam: Friday, March 13, 2026 (first half of class). Final Exam: Friday, May 8, 2026, 2:00pm–4:30pm (regular classroom). Students who choose to complete a final project may opt out of the final exam.
</p>

<hr />

<h2>Final Project</h2>
<p>
Project Proposal Due: Thursday, April 2, 2026. Guidelines: <a href="project_guidelines.pdf">here</a>
</p>

<hr />

<h2>Course Resources</h2>
<p>
There is no textbook to purchase. Course material will consist of my slides, lecture notes scribed by Teal Witter, as well as assorted online resources, including papers, notes from other courses, and publicly available surveys.
</p>

<hr />

<h2>Lecture Schedule</h2>
<p>
The following schedule is tentative and subject to change. Topics and resources will be updated throughout the semester.
</p>

<table>
  <tr>
    <th style="width: 8%;">Week</th>
    <th style="width: 12%;">Date</th>
    <th style="width: 45%;">Topic</th>
    <th style="width: 35%;">Resources</th>
  </tr>
  <tr>
    <td>1</td>
    <td>1/23</td>
    <td>Random variables, concentration, Markov's inequality <a href="lectures/lec1.pdf">[slides]</a> <a href="notes/lecture01.html">[notes]</a></td>
    <td class="small-text"><a href="http://cs229.stanford.edu/section/cs229-prob.pdf">Probability review</a>, <a href="http://probabilitycourse.com">Alternative resource</a>, <a href="https://edoliberty.github.io/papers/Estimating_the_size_of_OSN-WWW2011.pdf">Mark-and-recapture paper</a></td>
  </tr>
  <tr>
    <td>2</td>
    <td>1/30</td>
    <td>Efficient hashing, Chebyshev inequality</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec1.pdf">Princeton universality notes</a>, <a href="resources/FlajoletDurand.pdf">Flajolet-Durand paper</a></td>
  </tr>
  <tr>
    <td>3</td>
    <td>2/6</td>
    <td>Exponential tail bounds (Chernoff, Bernstein)</td>
    <td class="small-text"><a href="https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/">Terry Tao notes</a>, <a href="https://www.eecs.harvard.edu/~michaelm/postscripts/handbook2001.pdf">Power of two choices</a></td>
  </tr>
  <tr>
    <td>4</td>
    <td>2/13</td>
    <td>High-dimensional geometry</td>
    <td class="small-text"><a href="https://www.cs.cornell.edu/jeh/book.pdf">Foundations of Data Science (Ch. 2)</a></td>
  </tr>
  <tr>
    <td>5</td>
    <td>2/20</td>
    <td>Johnson-Lindenstrauss lemma, dimensionality reduction</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec10.pdf">Princeton JL notes</a>, <a href="https://www.cs.cmu.edu/~avrim/Randalgs11/lectures/lect0314.pdf">Anupam Gupta's notes</a></td>
  </tr>
  <tr>
    <td>6</td>
    <td>2/27</td>
    <td>High-dimensional nearest neighbor search, locality sensitive hashing</td>
    <td class="small-text"><a href="https://web.stanford.edu/class/cs246/slides/03-lsh.pdf">Stanford LSH notes</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec12.pdf">Indyk-Motwani analysis</a></td>
  </tr>
  <tr>
    <td>7</td>
    <td>3/6</td>
    <td>Gradient descent, projected gradient descent</td>
    <td class="small-text"><a href="https://web.stanford.edu/class/cs246/handouts/CS246_LinAlg_review.pdf">Stanford linear algebra</a>, <a href="https://people.csail.mit.edu/madry/6S978/">Mądry's notes</a>, <a href="https://ee227c.github.io/notes/ee227c-notes.pdf">Moritz Hardt's notes</a>, <a href="https://arxiv.org/pdf/1405.4980.pdf">Sébastien Bubeck's book</a></td>
  </tr>
  <tr>
    <td>8</td>
    <td>3/13</td>
    <td><strong>Midterm</strong> + GD continuation, second-order conditions</td>
    <td class="small-text">Second-order conditions notes </td>
  </tr>
  <tr>
    <td>—</td>
    <td>3/20</td>
    <td><strong>SPRING BREAK</strong> (no class)</td>
    <td>—</td>
  </tr>
  <tr>
    <td>9</td>
    <td>3/27</td>
    <td>Online and stochastic gradient descent</td>
    <td class="small-text"><a href="https://arxiv.org/pdf/1909.05207.pdf">Elad Hazan's book</a></td>
  </tr>
  <tr>
    <td>10</td>
    <td>4/3</td>
    <td>Center of gravity, ellipsoid method, LP relaxation</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec16.pdf">Princeton ellipsoid method</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec17.pdf">Interior point method</a>, <a href="https://convex-optimization.github.io/">Nisheeth Vishnoi's book</a></td>
  </tr>
  <tr>
    <td>11</td>
    <td>4/10</td>
    <td>Singular value decomposition, Krylov subspace methods</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec14.pdf">Princeton power method</a>, <a href="https://www.cs.cornell.edu/jeh/book.pdf">Foundations of Data Science (Ch. 3)</a>, <a href="https://infolab.stanford.edu/~ullman/mmds/ch11.pdf">Stanford notes</a></td>
  </tr>
  <tr>
    <td>12</td>
    <td>4/17</td>
    <td>Spectral graph theory, spectral clustering, stochastic block model</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec14.pdf">Princeton SBM notes</a>, <a href="https://www1.ind.ku.dk/complexLearning/zachary1977.pdf">Karate Club paper</a>, <a href="https://web.stanford.edu/class/cs168/l/l11.pdf">Stanford spectral graph</a></td>
  </tr>
  <tr>
    <td>13</td>
    <td>4/24</td>
    <td>SBM continuation, randomized numerical linear algebra, sketching for regression, ε-nets</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec11.pdf">Princeton sketching notes</a>, <a href="https://www2.cs.duke.edu/courses/spring07/cps296.2/scribe_notes/lecture09.pdf">ε-nets context</a></td>
  </tr>
  <tr>
    <td>14</td>
    <td>5/1</td>
    <td>Fast JL, sparse recovery, compressed sensing</td>
    <td class="small-text"><a href="https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf">FJLT paper</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall18/cos521/Lectures/lec21.pdf">Compressed sensing notes</a></td>
  </tr>
  <tr>
    <td>15</td>
    <td>5/8</td>
    <td><strong>FINAL EXAM</strong> (2:00pm)</td>
    <td>Final exam preparation materials</td>
  </tr>
</table>

<p class="small-text">
Note: Lecture notes, slides, and additional resources will be posted on Brightspace before each lecture.
</p>

<hr />

<h2>Course Links</h2>
<p>
Brightspace: TBA. Gradescope: TBA.
</p>

<hr />

<h2>Academic Integrity</h2>
<p>
All students are expected to follow NYU's academic integrity policies. Violations will be taken seriously and may result in failure of the course and/or disciplinary action by the university.
</p>

<hr />

<p style="text-align: center; margin-top: 2em;">
<a href="../index.html">← Back to Ainesh Bakshi's Homepage</a>
</p>

</body>
</html>
